{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":46986,"databundleVersionId":5019370,"sourceType":"competition"},{"sourceId":1497,"sourceType":"modelInstanceVersion","modelInstanceId":1265,"modelId":191}],"dockerImageVersionId":30380,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=toc></a>\n\n# <h1 style=\"padding: 35px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;background-position: 0px 0px; \"><span style='color:white'><b>NLP | Sentiment Analysis of Company Reviews</b></span></h1>\n\n\n<center>\n    <figure>\n        <img src=\"https://www.surveysensum.com/wp-content/uploads/2020/02/SENTIMENT-09-1.png\" alt =\"Sentiment Analysis\" style='width:80%;'>\n        <figcaption>\n            Source: <a href=\"https://www.surveysensum.com/customer-experience/sentiment-analysis/\">[surveysensum.com | Sentiment Analysis: A Thorough Guide For The Data Geek]</a>\n        </figcaption>\n    </figure>\n</center>\n\n### üéØ Objective\n\nThe objective for this notebook is to build a baseline model which is capable of predicting the sentiment of company reviews left by customers for the [Sentiment Analysis - Company Reviews Competition](https://www.kaggle.com/competitions/sentiment-analysis-company-reviews/overview).\n\n### ü§î So what exactly is Sentiment Analysis?\n\n**According to [ChatGPT](https://openai.com/blog/chatgpt/):**\n\n> Sentiment analysis, also known as opinion mining, is the process of analyzing and identifying the sentiment, attitude, or emotion expressed in a piece of text, such as a review or a social media post.\n>\n> It is commonly used in various industries, such as marketing, customer service, and politics, to understand people's opinions, preferences, and behavior. Sentiment analysis uses natural language processing (NLP) techniques and algorithms to analyze and classify text into different categories, such as positive, negative, or neutral.The process involves pre-processing the text, such as tokenization and stemming, to convert it into a format that can be analyzed by the algorithms.\n>\n>Sentiment analysis models can be trained on a dataset of labeled data, which contains examples of text with their corresponding sentiment labels. Alternatively, models can be trained using unsupervised learning techniques, which use clustering and other methods to classify the text without labeled data.\n>\n>Sentiment analysis can be performed on various types of text, such as social media posts, product reviews, news articles, and customer feedback. Overall, sentiment analysis is a useful tool for understanding people's opinions and attitudes, and can help businesses make data-driven decisions based on customer feedback.\n\n**For more information on Sentiment Analysis, see the following links:**\n> - [MonkeyLearn | Sentiment Analysis: A Definitive Guide](https://monkeylearn.com/sentiment-analysis/)\n> - [Thematic | Sentiment Analysis: Comprehensive Beginners Guide](https://getthematic.com/sentiment-analysis/)\n\n### üìÅ Dataset\nThe dataset for this competition (both train and test) consists of 100,000 reviews collected from Trustpilot and spans over 40 different companies.\n\n> Find this competitions dataset here: [Sentiment Analysis - Company Reviews Dataset](https://www.kaggle.com/competitions/sentiment-analysis-company-reviews/data)\n\n<br>\n\n## Table of contents\n\n- [1 | Dataset Exploration](#1)\n   > - [Load CSV Files](#1.1)\n   > - [View Random Selected Samples](#1.2)\n   > - [View Train Rating Distribution](#1.3)\n   > - [Inspect Review Lengths & Tokens](#1.4)\n   > - [View Review Lengths & Review Token Count Histograms](#1.5)\n  \n- [2 | Data Preprocessing](#2)\n   > - [Label Encode Ratings](#2.1)\n   > - [Create Train/Validation Split](#2.2)\n   > - [View New Train & Validation Labels Distribution](#2.3)\n   \n- [3 | Build Input Data Pipeline with tf.data API](#3)\n   > - [Define Text Preprocessor](#3.1)\n   > - [Generate Input Data Pipelines](#3.2)\n   \n- [4 | Baseline Model: Universal Sentence Encoder Model](#4)\n   > - [TensorFlow Hub](#tfhub)\n   > - [Get Universal Sentence Encoder](#4.1)\n   > - [Build Model](#4.2)\n\n- [5 | Train Baseline Model](#5)\n   > - [Define Callbacks and Metrics for Model Training](#5.1)\n   > - [Compile & Train Model](#5.2)\n   \n- [6 | Model Performance Evaluation](#6)\n   > - [View Model Histories](#6.1)\n   > - [Plot Confusion Matrix](#6.2)\n   > - [Generate Classification Reports](#6.3)\n   > - [Record Classification Metrics](#6.4)\n   \n- [7 | Generate Submission](#7)\n   > - [Preprocess Test Reviews](#7.1)\n   > - [Generate Test Predictions](#7.2)\n   > - [Generate Submission.csv](#7.3)\n   \n- [Conclusion](#conclusion)\n\n<br>\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>","metadata":{}},{"cell_type":"code","source":"import re\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import top_k_accuracy_score, classification_report, precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\nfrom sklearn.metrics import confusion_matrix, mean_absolute_error\nfrom scikitplot.metrics import plot_roc","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:55:34.729872Z","iopub.execute_input":"2025-03-24T02:55:34.730379Z","iopub.status.idle":"2025-03-24T02:55:43.061808Z","shell.execute_reply.started":"2025-03-24T02:55:34.730250Z","shell.execute_reply":"2025-03-24T02:55:43.060598Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\nplt.style.use('dark_background')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T02:55:48.316842Z","iopub.execute_input":"2025-03-24T02:55:48.317364Z","iopub.status.idle":"2025-03-24T02:55:48.323750Z","shell.execute_reply.started":"2025-03-24T02:55:48.317320Z","shell.execute_reply":"2025-03-24T02:55:48.322474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    SEED = 768\n    BATCH_SIZE = 32\n    EPOCHS = 10","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:55:49.234421Z","iopub.execute_input":"2025-03-24T02:55:49.234813Z","iopub.status.idle":"2025-03-24T02:55:49.239945Z","shell.execute_reply.started":"2025-03-24T02:55:49.234783Z","shell.execute_reply":"2025-03-24T02:55:49.238685Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed=CFG.SEED):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(CFG.SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T02:55:51.137122Z","iopub.execute_input":"2025-03-24T02:55:51.137550Z","iopub.status.idle":"2025-03-24T02:55:51.143713Z","shell.execute_reply.started":"2025-03-24T02:55:51.137514Z","shell.execute_reply":"2025-03-24T02:55:51.142367Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='1'></a>\n# 1 | Dataset Exploration\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>","metadata":{}},{"cell_type":"code","source":"# Define paths\nDATASET_PATH = \"/kaggle/input/sentiment-analysis-company-reviews/\"\nTRAIN_CSV = '/kaggle/input/sentiment-analysis-company-reviews/train.csv'\nTEST_CSV = '/kaggle/input/sentiment-analysis-company-reviews/test.csv'\nSAMPLE_SUB_CSV = '/kaggle/input/sentiment-analysis-company-reviews/sample_submission.csv'","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:55:52.218818Z","iopub.execute_input":"2025-03-24T02:55:52.219230Z","iopub.status.idle":"2025-03-24T02:55:52.225615Z","shell.execute_reply.started":"2025-03-24T02:55:52.219195Z","shell.execute_reply":"2025-03-24T02:55:52.224066Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load CSV Files","metadata":{}},{"cell_type":"code","source":"# Load the csv files\ntrain_df = pd.read_csv(TRAIN_CSV)\ntest_df = pd.read_csv(TEST_CSV)\nsubmission_df = pd.read_csv(SAMPLE_SUB_CSV) ","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:55:54.435168Z","iopub.execute_input":"2025-03-24T02:55:54.436202Z","iopub.status.idle":"2025-03-24T02:55:55.372699Z","shell.execute_reply.started":"2025-03-24T02:55:54.436162Z","shell.execute_reply":"2025-03-24T02:55:55.371132Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate summary of the training set\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:55:55.374976Z","iopub.execute_input":"2025-03-24T02:55:55.375467Z","iopub.status.idle":"2025-03-24T02:55:55.415271Z","shell.execute_reply.started":"2025-03-24T02:55:55.375421Z","shell.execute_reply":"2025-03-24T02:55:55.414129Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View first 5 training samples\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:55:56.997050Z","iopub.execute_input":"2025-03-24T02:55:56.997481Z","iopub.status.idle":"2025-03-24T02:55:57.018491Z","shell.execute_reply.started":"2025-03-24T02:55:56.997448Z","shell.execute_reply":"2025-03-24T02:55:57.017453Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='1.2'></a>\n### View Random Selected Samples","metadata":{}},{"cell_type":"code","source":"def view_samples(df, count=5):\n    idx = random.sample(train_df.index.to_list(), count)\n    print('=========================================\\n')\n    for _ in idx:\n        print(f'id:\\t{df.Id[_]}\\n')\n        print(f'Review:\\n{df.Review[_]}\\n')\n        print(f'Rating:\\n{df.Rating[_]}')\n        print('=========================================\\n')","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:55:59.525077Z","iopub.execute_input":"2025-03-24T02:55:59.525475Z","iopub.status.idle":"2025-03-24T02:55:59.531931Z","shell.execute_reply.started":"2025-03-24T02:55:59.525441Z","shell.execute_reply":"2025-03-24T02:55:59.530735Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View 5 randomly selected samples\nview_samples(train_df, count=5)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:55:59.958424Z","iopub.execute_input":"2025-03-24T02:55:59.959468Z","iopub.status.idle":"2025-03-24T02:55:59.970048Z","shell.execute_reply.started":"2025-03-24T02:55:59.959433Z","shell.execute_reply":"2025-03-24T02:55:59.968788Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='1.3'></a>\n### View Train Rating Distribution","metadata":{}},{"cell_type":"code","source":"# View Train Rating Distribution\nplt.figure(figsize=(15, 8))\nplt.title('Train Rating Distribution', fontsize=20)\n\ntrain_distribution = train_df['Rating'].value_counts().sort_values()\nsns.barplot(x=list(train_distribution.keys()),\n            y=train_distribution.values);\n\nsns.despine();","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:05.056604Z","iopub.execute_input":"2025-03-24T02:56:05.057480Z","iopub.status.idle":"2025-03-24T02:56:05.306417Z","shell.execute_reply.started":"2025-03-24T02:56:05.057439Z","shell.execute_reply":"2025-03-24T02:56:05.305249Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<br>\nWe observe that <b>the dataset is severly imbalanced</b> with review with ratings of 1 and 5 make up the majority of the dataset's reviews. Techniques such as undersampling, oversampling or weighted training should be considered. However, this notebook will only focus on the baseline model and will not cover the implementation of these techniques. It should be noted that this behavior in the data is most likely representative of customer behavior (customers would rather give a rating of 1 or 5 and ratings in between siginify mixed opinions). \n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id='1.4'></a>\n### Inspect Review Lengths & Tokens","metadata":{}},{"cell_type":"code","source":"# Get the lengths of each review\ntrain_df['review_length'] = [len(_) for _ in train_df.Review]\n\n# Get the number of tokens per review \ntrain_df['token_count'] = [len(_.split()) for _ in train_df.Review]","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:08.468537Z","iopub.execute_input":"2025-03-24T02:56:08.468936Z","iopub.status.idle":"2025-03-24T02:56:08.757685Z","shell.execute_reply.started":"2025-03-24T02:56:08.468908Z","shell.execute_reply":"2025-03-24T02:56:08.756706Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View first 5 samples \ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:10.526541Z","iopub.execute_input":"2025-03-24T02:56:10.526920Z","iopub.status.idle":"2025-03-24T02:56:10.537778Z","shell.execute_reply.started":"2025-03-24T02:56:10.526890Z","shell.execute_reply":"2025-03-24T02:56:10.536584Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect Review Length Stats\nprint('Review Length Description')\nprint('==================================')\nprint(train_df['review_length'].describe())\nprint('==================================')","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:11.028440Z","iopub.execute_input":"2025-03-24T02:56:11.028856Z","iopub.status.idle":"2025-03-24T02:56:11.044536Z","shell.execute_reply.started":"2025-03-24T02:56:11.028821Z","shell.execute_reply":"2025-03-24T02:56:11.043374Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect Token Count Stats\nprint('Token Count Description')\nprint('==================================')\nprint(train_df['token_count'].describe())\nprint('==================================')","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:11.700415Z","iopub.execute_input":"2025-03-24T02:56:11.700825Z","iopub.status.idle":"2025-03-24T02:56:11.713928Z","shell.execute_reply.started":"2025-03-24T02:56:11.700791Z","shell.execute_reply":"2025-03-24T02:56:11.712611Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, figsize=(14, 18))\n\n# Set the spacing between subplots\nfig.tight_layout(pad=6.0)\n\n# Plot Range of Review Lengths per Rating\nax1.set_title('Review Lengths per Rating', fontsize=20)\nsns.boxplot(data=train_df, y='review_length', x='Rating',\n            ax=ax1)\nax1.set_xlabel('Rating', fontsize=14)\nax1.set_ylabel('review_length', fontsize=14)\nsns.despine();\n\n# Plot Range of Token Counts per Rating\nax2.set_title('Token Counts per Rating', fontsize=20)\nsns.boxplot(data=train_df, y='token_count', x='Rating',\n            ax=ax2);\nax2.set_xlabel('Rating', fontsize=14)\nax2.set_ylabel('token_count', fontsize=14)\nsns.despine();","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:13.981262Z","iopub.execute_input":"2025-03-24T02:56:13.981647Z","iopub.status.idle":"2025-03-24T02:56:14.562035Z","shell.execute_reply.started":"2025-03-24T02:56:13.981616Z","shell.execute_reply":"2025-03-24T02:56:14.560845Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<br>\nWe observe that the length of the reviews increase the more unsatisfied the customers are with the companies. The same observation can be made for the number of tokens per review. The reason for this may be that customers tend to explain or describe their opinions/experiences in great detail the more unsatisfied they are with the companies. This reason may also explain why reviews with higher ratings are generally shorter with less tokens present as this signifies satification amongst customers.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id='1.5'></a>\n### View Review Lengths & Review Token Count Histograms","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, figsize=(14, 10))\n\n# Set the spacing between subplots\nfig.tight_layout(pad=6.0)\n\n# Generate Train Rating Histogram\nax1.set_title('Train Review Length Histogram', fontsize=20)\nsns.histplot(data=train_df, x='review_length', bins=50,\n            ax=ax1)\nax1.set_xlabel('review_length', fontsize=14)\nax1.set_ylabel('Count', fontsize=14)\nsns.despine();\n\n# Generate Train Token Count Histogram\nax2.set_title('Train Token Count Histogram', fontsize=20)\nsns.histplot(data=train_df, x='token_count', bins=50,\n            ax=ax2)\nax2.set_xlabel('token_count', fontsize=14)\nax2.set_ylabel('Count', fontsize=14)\nsns.despine();","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:18.958139Z","iopub.execute_input":"2025-03-24T02:56:18.959025Z","iopub.status.idle":"2025-03-24T02:56:19.585876Z","shell.execute_reply.started":"2025-03-24T02:56:18.958985Z","shell.execute_reply":"2025-03-24T02:56:19.584648Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<br>\nWe observe that the majority of review lengths are under a length of ~1000. We also observe that the review token counts are generally under ~300 tokens. These factors should be considered when selecting the number of tokens to be used in a model. Selecting the number of tokens to be used in model via percentiles may prove to be helpful. However, this will not be covered in this notebook.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#03e3fc;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n\n<a id='2'></a>\n# 2 | Data Preprocessing\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>\n\n<a id='2.1'></a>\n### Label Encode Ratings\n\nWe need to label encode the ratings since all ratings fall in the range between 1 and 5 inclusively. To achieve this we simply shift the ratings by subtracting 1 from each rating (e.g. 5 -> 4). We do this in order to simplify the one-hot encoding process at a later stage.","metadata":{}},{"cell_type":"code","source":"# Label encode ratings\ntrain_df[\"rating_encoded\"] = train_df['Rating'] - 1","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:23.583417Z","iopub.execute_input":"2025-03-24T02:56:23.584283Z","iopub.status.idle":"2025-03-24T02:56:23.590566Z","shell.execute_reply.started":"2025-03-24T02:56:23.584240Z","shell.execute_reply":"2025-03-24T02:56:23.589243Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='2.2'></a>\n### Create Train/Validation Split","metadata":{}},{"cell_type":"code","source":"# Get indices for train and validation splits\ntrain_idx, val_idx, _, _ = train_test_split(\n    train_df.index, train_df.Rating, \n    test_size=0.2, stratify=train_df.Rating,\n    random_state=CFG.SEED\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:26.287574Z","iopub.execute_input":"2025-03-24T02:56:26.287982Z","iopub.status.idle":"2025-03-24T02:56:26.316498Z","shell.execute_reply.started":"2025-03-24T02:56:26.287951Z","shell.execute_reply":"2025-03-24T02:56:26.315261Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get new training and validation data\ntrain_new_df = train_df.iloc[train_idx].reset_index(drop=True)\nval_df = train_df.iloc[val_idx].reset_index(drop=True)\n\n# View shapes\ntrain_new_df.shape, val_df.shape","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:26.677268Z","iopub.execute_input":"2025-03-24T02:56:26.678412Z","iopub.status.idle":"2025-03-24T02:56:26.701908Z","shell.execute_reply.started":"2025-03-24T02:56:26.678368Z","shell.execute_reply":"2025-03-24T02:56:26.700759Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View new train dataframe\ntrain_new_df","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:29.564785Z","iopub.execute_input":"2025-03-24T02:56:29.565154Z","iopub.status.idle":"2025-03-24T02:56:29.581189Z","shell.execute_reply.started":"2025-03-24T02:56:29.565125Z","shell.execute_reply":"2025-03-24T02:56:29.580092Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='2.3'></a>\n### View Train & Validation Rating Distributions","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, figsize=(14, 10))\n\n# Set the spacing between subplots\nfig.tight_layout(pad=6.0)\n\n# Plot New Train Ratings Distribution\nax1.set_title('New Train Ratings Distribution', fontsize=20)\ntrain_new_distribution = train_new_df['Rating'].value_counts().sort_values()\nsns.barplot(x=train_new_distribution.values,\n            y=list(train_new_distribution.keys()),\n            orient=\"h\",\n            ax=ax1)\nsns.despine();\n\n# Plot Validation Ratings Distribution\nax2.set_title('Validation Ratings Distribution', fontsize=20)\nval_distribution = val_df['Rating'].value_counts().sort_values()\nsns.barplot(x=val_distribution.values,\n            y=list(val_distribution.keys()),\n            orient=\"h\",\n            ax=ax2);\nsns.despine();","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:32.466319Z","iopub.execute_input":"2025-03-24T02:56:32.466734Z","iopub.status.idle":"2025-03-24T02:56:32.832402Z","shell.execute_reply.started":"2025-03-24T02:56:32.466702Z","shell.execute_reply":"2025-03-24T02:56:32.831215Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#03e3fc;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n           \n<a id='3'></a>\n# 3 | Build Input Data Pipeline with tf.data API\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>\n<br>\n\nIn this notebook we'll use the tf.data API to build input data pipelines for training a model and conducting model inference. In order to achieve this, we'll preprocess the reviews by removing any artifacts in the texts such as emojis, non-ascii characters and replacing numbers with another character. The preprocessed texts will be used to construct the pipelines along with the one-hot encoded ratings.\n\n<center>\n    <figure>\n        <img src=\"https://i.postimg.cc/GpPNLLNH/1-3-Zi3-VUAYh5w78-KGLo-O37g.webp\" alt =\"UCF101\" style='width: 800px;'>\n        <figcaption>\n            Source: <a href=\"https://towardsdatascience.com/how-to-reduce-training-time-for-a-deep-learning-model-using-tf-data-43e1989d2961\">[Medium | How to Reduce Training Time for a Deep Learning Model using tf.data]</a>\n        </figcaption>\n    </figure>\n</center>\n\n<br>\n\n**For more information on the tf.data API and loading data from generator, follow these links:** \n> - [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data)\n> - [Better performance with the tf.data API](https://www.tensorflow.org/guide/data_performance)\n> - [Using generators with tf.data API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator)","metadata":{}},{"cell_type":"markdown","source":"<a id='3.1'></a>\n### Define Text Preprocessor","metadata":{}},{"cell_type":"code","source":"def text_preprocessor(text):\n    \n    # -----------------------------------------------------\n    # Source: https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n    emoji_pattern = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags=re.UNICODE)\n    # -----------------------------------------------------\n    non_ascii_pattern = re.compile(r\"[^\\x00-\\x7F]+\", flags=re.UNICODE)\n    digit_pattern = re.compile('[0-9]', flags=re.UNICODE)\n    \n    # -----------------------------------------------------\n    # Source: https://stackoverflow.com/questions/21932615/regular-expression-for-remove-link\n    link_pattern = re.compile('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)', flags=re.UNICODE)\n    # -----------------------------------------------------\n    \n    # Remove emojis\n    preprocessed_text = emoji_pattern.sub(r'', text)\n    # Remoce non-ascii characters\n    preprocessed_text = non_ascii_pattern.sub(r'', preprocessed_text)\n    # Replace numbers with '@' sign\n    preprocessed_text = digit_pattern.sub(r'#', preprocessed_text)\n    # Remove web links \n    preprocessed_text = link_pattern.sub(r'', preprocessed_text)\n    \n    return preprocessed_text","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:35.235158Z","iopub.execute_input":"2025-03-24T02:56:35.235863Z","iopub.status.idle":"2025-03-24T02:56:35.243561Z","shell.execute_reply.started":"2025-03-24T02:56:35.235826Z","shell.execute_reply":"2025-03-24T02:56:35.242280Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='3.2'></a>\n### Generate Input Data Pipelines","metadata":{}},{"cell_type":"code","source":"def encode_labels(labels, label_depth=5):\n    return tf.one_hot(labels, depth=label_depth).numpy()\n\ndef create_pipeline(df, preprocessor, batch_size=32, shuffle=False, cache=None, prefetch=False):\n    '''\n    Generates an input pipeline using the tf.data API given a Pandas DataFrame and image loading function.\n    \n    @params\n        - df: (pd.DataFrame) -> DataFrame containing texts and labels\n        - preprocessor (function) -> preprocessor used to preprocess texts\n        - batch_size: (int) -> size for batched (default=32) \n        - shuffle: (bool) -> condition for data shuffling, data is shuffled when True (default=False)\n        - cache: (str) -> cache path for caching data, data is not cached when None (default=None)\n        - prefetch: (bool) -> condition for prefeching data, data is prefetched when True (default=False)\n        \n    @returns\n        - dataset: (tf.data.Dataset) -> dataset input pipeline used to train a TensorFlow model\n    '''\n    # Get image paths and labels from DataFrame\n    reviews = df['Review'].apply(preprocessor).to_numpy().astype(str)\n    ratings = encode_labels(df['rating_encoded'].to_numpy().astype(np.float32))\n    AUTOTUNE = tf.data.AUTOTUNE\n    \n    # Create dataset with raw data from DataFrame\n    ds = tf.data.Dataset.from_tensor_slices((reviews, ratings))\n    \n    # Apply shuffling based on condition\n    if shuffle:\n        ds = ds.shuffle(buffer_size=1000)\n        \n    # Apply batching\n    ds = ds.batch(batch_size)\n    \n    # Apply caching based on condition\n    # Note: Use cache in memory (cache='') if the data is small enough to fit in memory!!!\n    if cache != None:\n        ds = ds.cache(cache)\n    \n    # Apply prefetching based on condition\n    # Note: This will result in memory trade-offs\n    if prefetch:\n        ds = ds.prefetch(buffer_size=AUTOTUNE)\n    \n    # Return the dataset\n    return ds","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:37.906354Z","iopub.execute_input":"2025-03-24T02:56:37.906728Z","iopub.status.idle":"2025-03-24T02:56:37.916352Z","shell.execute_reply.started":"2025-03-24T02:56:37.906698Z","shell.execute_reply":"2025-03-24T02:56:37.915040Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create train input data pipeline\ntrain_ds = create_pipeline(\n    train_new_df, text_preprocessor, \n    batch_size=CFG.BATCH_SIZE, \n    shuffle=False, prefetch=True\n)\n\n# Create validation input data pipeline\nval_ds = create_pipeline(\n    val_df, text_preprocessor,\n    batch_size=CFG.BATCH_SIZE, \n    shuffle=False, prefetch=False\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:40.628624Z","iopub.execute_input":"2025-03-24T02:56:40.628997Z","iopub.status.idle":"2025-03-24T02:56:46.707081Z","shell.execute_reply.started":"2025-03-24T02:56:40.628967Z","shell.execute_reply":"2025-03-24T02:56:46.705990Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View string representation of datasets\nprint('========================================')\nprint('Train Input Data Pipeline:\\n\\n', train_ds)\nprint('========================================')\nprint('Validation Input Data Pipeline:\\n\\n', val_ds)\nprint('========================================')","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:46.709054Z","iopub.execute_input":"2025-03-24T02:56:46.709425Z","iopub.status.idle":"2025-03-24T02:56:46.716520Z","shell.execute_reply.started":"2025-03-24T02:56:46.709389Z","shell.execute_reply":"2025-03-24T02:56:46.715144Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#03e3fc;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n\n<a id='4'></a>\n# 4 | Baseline Model: Universal Sentence Encoder Model\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>\n<br>\n\nThe Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering, and other natural language tasks. For this baseline model we'll make use of Universal Sentence Encoder (USE) to generate embeddings which are representative of the review texts.\n\n<center>\n    <figure>\n        <img src=\"https://amitness.com/posts/images/use-overall-pipeline.png\" alt =\"USE Diagram\" style='width: 600px;'>\n        <figcaption>\n            Source: <a href=\"https://amitness.com/posts/universal-sentence-encoder\">[Amit Chaudhary | Universal Sentence Encoder Visually Explained]</a>\n        </figcaption>\n    </figure>\n</center>\n\n**Reasons for using Universal Sentence Encoder:**\n1. Minimal hardware requirements for generating embeddings with USE\n2. Low inference rate\n3. Light-weight memory consumptions\n\n**Drawbacks of using Universal Sentence Encoder:**\n1. Embedding representations become less accurate as text lengths increases.\n2. Although USE has a low inference rate, its accuracy falls short when compared to language models such as BERT & DeBERTa-v3\n\n<br>\n\n**For more information regarding the Universal Sentence Encoder, follow these links:** \n> - [An Introduction to Transfer Learning](https://georgian.io/an-introduction-to-transfer-learning/)\n> - [A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)\n> - <a href=\"https://amitness.com/2020/06/universal-sentence-encoder/\">Amit Chaudhary | Universal Sentence Encoder Visually Explained</a>\n\n<br>\n\n<a id=\"tfhub\"></a>\n## TensorFlow Hub\n\nTensorFlow Hub is a repository of trained machine learning models ready for fine-tuning and deployable anywhere. TensorFlow Hub enables us to reuse trained models like BERT and Faster R-CNN with just a few lines of code. In this section we'll get the USE model from TensorFlow Hub. \n\n**For more information on TensorFlow Hub or if you would like to access the other models in PyTorch/JAX, check out the following links:**\n> - [TensorFlow Hub](https://www.tensorflow.org/hub)\n> - [HuggingFaceü§ó](https://huggingface.co/)","metadata":{}},{"cell_type":"code","source":"# Here's a function to get any model/preprocessor from tensorflow hub\ndef get_tfhub_model(model_link, model_name, model_trainable=False):\n    return hub.KerasLayer(model_link,\n                          trainable=model_trainable,\n                          name=model_name)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:46.718110Z","iopub.execute_input":"2025-03-24T02:56:46.718529Z","iopub.status.idle":"2025-03-24T02:56:46.728507Z","shell.execute_reply.started":"2025-03-24T02:56:46.718495Z","shell.execute_reply":"2025-03-24T02:56:46.727238Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='4.1'></a>\n### Get Universal Sentence Encoder","metadata":{}},{"cell_type":"code","source":"# Get Universal Sentence Encoder here\n# -----------------------------------\n# Note: We'll use the version from Kaggle's Models page instead.\n#       Check it out here: \n#       (https://www.kaggle.com/models/google/universal-sentence-encoder)\n# -----------------------------------\nencoder_link = 'https://kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2'\n# encoder_link = 'https://tfhub.dev/google/universal-sentence-encoder/4'\n\nencoder_name = 'universal_sentence_encoder'\nencoder_trainable=False # set trainable to False for inference-only \n\nencoder = get_tfhub_model(encoder_link, encoder_name, model_trainable=encoder_trainable)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:56:49.797410Z","iopub.execute_input":"2025-03-24T02:56:49.798522Z","iopub.status.idle":"2025-03-24T02:57:28.650273Z","shell.execute_reply.started":"2025-03-24T02:56:49.798481Z","shell.execute_reply":"2025-03-24T02:57:28.649166Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='4.2'></a>\n### Build Model","metadata":{}},{"cell_type":"code","source":"def build_baseline_model(num_classes=5):\n    # Define kernel initializer & input layer\n    initializer = tf.keras.initializers.HeNormal(seed=CFG.SEED)\n    review_input = layers.Input(shape=[], dtype=tf.string, name='review_text_input')\n    \n    # Generate Embeddings\n    review_embedding = encoder(review_input)\n    \n    # Feed Embeddings to a Bidirectional LSTM\n    expand_layer = layers.Lambda(lambda embed: tf.expand_dims(embed, axis=1))(review_embedding)\n    bi_lstm = layers.Bidirectional(layers.LSTM(128, kernel_initializer=initializer), \n                                   name='bidirection_lstm')(expand_layer)\n    \n    # Feed LSTM output to classification head\n    dropout_layer = layers.Dropout(0.25)(bi_lstm)\n    dense_layer = layers.Dense(64, activation='relu', kernel_initializer=initializer)(dropout_layer)\n    output_layer = layers.Dense(num_classes, activation='softmax', \n                                kernel_initializer=initializer, \n                                name='output_layer')(dense_layer)\n    \n    return tf.keras.Model(inputs=[review_input], \n                          outputs=[output_layer], \n                          name='use_model')","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:57:28.651976Z","iopub.execute_input":"2025-03-24T02:57:28.652290Z","iopub.status.idle":"2025-03-24T02:57:28.661482Z","shell.execute_reply.started":"2025-03-24T02:57:28.652262Z","shell.execute_reply":"2025-03-24T02:57:28.660256Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build model\nmodel = build_baseline_model()\n\n# View summary of model\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:57:28.663009Z","iopub.execute_input":"2025-03-24T02:57:28.663382Z","iopub.status.idle":"2025-03-24T02:57:31.179815Z","shell.execute_reply.started":"2025-03-24T02:57:28.663350Z","shell.execute_reply":"2025-03-24T02:57:31.178673Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Explore model visually\nplot_model(\n    model, dpi=60,\n    show_shapes=True,\n    expand_nested=True\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:57:40.014602Z","iopub.execute_input":"2025-03-24T02:57:40.015006Z","iopub.status.idle":"2025-03-24T02:57:41.361729Z","shell.execute_reply.started":"2025-03-24T02:57:40.014972Z","shell.execute_reply":"2025-03-24T02:57:41.360406Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#03e3fc;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n           \n<a id='5'></a>\n# 5 | Train Baseline Model\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>\n<br>\n\nTo train this model we'll use Categorical Crossentropy as the loss function since this notebook approaches the problem at hand as a classification problem for multiple labels. As for the optimizer, we'll use the Adam optimizer with 0.001 as the (default) learning rate. \n\nTo prevent the occurance of overfitting during training we'll have to make use of TensorFlow's Callback API to implement the EarlyStopping & ReduceLROnPlateau callbacks. The only metrics we'll track during the training of the model will be the loss and accuracy metrics.\n\n**See the following for more information:**\n> - **Categorical Crossentropy Loss Function:**\n>    - [Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names](https://gombru.github.io/2018/05/23/cross_entropy_loss/)\n>    - [TensorFlow Categorical Crossentropy Loss Implementation](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy)\n> - **Adam Optimizer:**\n>    - [Academic Paper | Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n>     - [TensorFlow Adam Implementation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n> - **TensorFlow Callback API:**\n>    - [EarlyStopping Implementation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)\n>    - [ReduceLROnPlateau Implementation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau)\n> - **TensorFlow Metrics:**\n>    - [TensorFlow Metrics Overview](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)","metadata":{}},{"cell_type":"code","source":"def train_model(model, num_epochs, callbacks_list, tf_train_data, \n                tf_valid_data=None, shuffling=False):\n    '''\n        Trains a TensorFlow model and returns a dict object containing the model metrics history data. \n        \n        @params\n        - model: (tf.keras.model) -> model to be trained \n        - num_epochs: (int) -> number of epochs to train the model\n        - callbacks_list: (list) -> list containing callback fuctions for model\n        - tf_train_data: (tf.data.Dataset) -> dataset for model to be train on \n        - tf_valid_data: (tf.data.Dataset) -> dataset for model to be validated on (default=None)\n        - shuffling: (bool) -> condition for data shuffling, data is shuffled when True (default=False)\n        \n        @returns\n        - model_history: (dict) -> dictionary containing loss and metrics values tracked during training\n    '''\n    \n    model_history = {}\n    \n    if tf_valid_data != None:\n        model_history = model.fit(tf_train_data,\n                                  epochs=num_epochs,\n                                  validation_data=tf_valid_data,\n                                  validation_steps=int(len(tf_valid_data)),\n                                  callbacks=callbacks_list,\n                                  shuffle=shuffling)\n        \n    if tf_valid_data == None:\n        model_history = model.fit(tf_train_data,\n                                  epochs=num_epochs,\n                                  callbacks=callbacks_list,\n                                  shuffle=shuffling)\n    return model_history","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:57:57.074719Z","iopub.execute_input":"2025-03-24T02:57:57.075146Z","iopub.status.idle":"2025-03-24T02:57:57.085650Z","shell.execute_reply.started":"2025-03-24T02:57:57.075106Z","shell.execute_reply":"2025-03-24T02:57:57.084153Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=5.1></a>\n### Define Callbacks and Metrics for Model Training","metadata":{}},{"cell_type":"code","source":"early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    patience=4, \n    restore_best_weights=True)\n\nreduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    patience=2,\n    factor=0.1,\n    verbose=1)\n\nCALLBACKS = [early_stopping_callback, reduce_lr_callback]\nMETRICS = ['accuracy']","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:58:01.867410Z","iopub.execute_input":"2025-03-24T02:58:01.867787Z","iopub.status.idle":"2025-03-24T02:58:01.874572Z","shell.execute_reply.started":"2025-03-24T02:58:01.867759Z","shell.execute_reply":"2025-03-24T02:58:01.873275Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=5.2></a>\n### Compile & Train Model","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(CFG.SEED)\n\nmodel.compile(\n    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    metrics=METRICS\n)\n\nprint(f'Training {model.name}.')\nprint(f'Train on {len(train_new_df)} samples, validate on {len(val_df)} samples.')\nprint('----------------------------------')\n\nmodel_history = train_model(\n    model, CFG.EPOCHS, CALLBACKS, \n    train_ds, val_ds,\n    shuffling=False\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T02:58:05.828733Z","iopub.execute_input":"2025-03-24T02:58:05.829256Z","iopub.status.idle":"2025-03-24T03:06:39.485534Z","shell.execute_reply.started":"2025-03-24T02:58:05.829212Z","shell.execute_reply":"2025-03-24T03:06:39.483937Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model\nmodel_evaluation = model.evaluate(val_ds)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:06:39.489462Z","iopub.execute_input":"2025-03-24T03:06:39.490598Z","iopub.status.idle":"2025-03-24T03:06:47.813837Z","shell.execute_reply.started":"2025-03-24T03:06:39.490557Z","shell.execute_reply":"2025-03-24T03:06:47.812542Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate model probabilities and associated predictions\ntrain_probabilities = model.predict(train_ds, verbose=1)\ntrain_predictions = tf.argmax(train_probabilities, axis=1)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:08:20.106823Z","iopub.execute_input":"2025-03-24T03:08:20.107234Z","iopub.status.idle":"2025-03-24T03:08:58.780024Z","shell.execute_reply.started":"2025-03-24T03:08:20.107202Z","shell.execute_reply":"2025-03-24T03:08:58.778692Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate model probabilities and associated predictions\nval_probabilities = model.predict(val_ds, verbose=1)\nval_predictions = tf.argmax(val_probabilities, axis=1)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:09:25.333273Z","iopub.execute_input":"2025-03-24T03:09:25.333736Z","iopub.status.idle":"2025-03-24T03:09:33.757539Z","shell.execute_reply.started":"2025-03-24T03:09:25.333701Z","shell.execute_reply":"2025-03-24T03:09:33.756012Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#03e3fc;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n           \n<a id='6'></a>\n# 6 | Model Performance Evaluation\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>\n<br>\n\nNow that the model has trained on the data we need to inspect how well it performs on the validation data. In order to conduct this inspection we need to evaluate the performance of the model on the validation data and record evaluation metrics. Since the approach for this problem is a multi classification problem we'll make use of some well known classification metrics. Hence, we'll make use of the Scikit Learn library to inspect the model. We'll also use the following to inspect the model:\n\n> - [Classification Report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)\n> - [Accuracy Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n> - [Precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)\n> - [Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)\n> - [F1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) \n> - [Matthews Correlation Coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)","metadata":{}},{"cell_type":"markdown","source":"<a id='6.1'></a>\n### Plot Model Training History","metadata":{}},{"cell_type":"code","source":"def plot_training_curves(history):\n    \n    loss = np.array(history.history['loss'])\n    val_loss = np.array(history.history['val_loss'])\n\n    accuracy = np.array(history.history['accuracy'])\n    val_accuracy = np.array(history.history['val_accuracy'])\n\n    epochs = range(len(history.history['loss']))\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n\n    # Plot loss\n    ax1.plot(epochs, loss, label='training_loss', marker='o')\n    ax1.plot(epochs, val_loss, label='val_loss', marker='o')\n    \n    ax1.fill_between(epochs, loss, val_loss, where=(loss > val_loss), color='C0', alpha=0.3, interpolate=True)\n    ax1.fill_between(epochs, loss, val_loss, where=(loss < val_loss), color='C1', alpha=0.3, interpolate=True)\n\n    ax1.set_title('Loss (Lower Means Better)', fontsize=16)\n    ax1.set_xlabel('Epochs', fontsize=14)\n    ax1.legend()\n    sns.despine();\n\n    # Plot accuracy\n    ax2.plot(epochs, accuracy, label='training_accuracy', marker='o')\n    ax2.plot(epochs, val_accuracy, label='val_accuracy', marker='o')\n    \n    ax2.fill_between(epochs, accuracy, val_accuracy, where=(accuracy > val_accuracy), color='C0', alpha=0.3, interpolate=True)\n    ax2.fill_between(epochs, accuracy, val_accuracy, where=(accuracy < val_accuracy), color='C1', alpha=0.3, interpolate=True)\n\n    ax2.set_title('Accuracy (Higher Means Better)', fontsize=16)\n    ax2.set_xlabel('Epochs', fontsize=14)\n    ax2.legend();\n    sns.despine();","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:09:40.723629Z","iopub.execute_input":"2025-03-24T03:09:40.724097Z","iopub.status.idle":"2025-03-24T03:09:40.739035Z","shell.execute_reply.started":"2025-03-24T03:09:40.724059Z","shell.execute_reply":"2025-03-24T03:09:40.737517Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot model training history \nplot_training_curves(model_history)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:09:43.204567Z","iopub.execute_input":"2025-03-24T03:09:43.205564Z","iopub.status.idle":"2025-03-24T03:09:43.625572Z","shell.execute_reply.started":"2025-03-24T03:09:43.205522Z","shell.execute_reply":"2025-03-24T03:09:43.624355Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<br>\nWe observe that overfitting may have occured during the first few expochs. We also observe that the model reached a plateau on the validation loss.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id='6.2'></a>\n### Plot Confusion Matrix","metadata":{}},{"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, classes='auto', figsize=(10, 10), text_size=12): \n    # Generate confusion matrix \n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Set plot size\n    plt.figure(figsize=figsize)\n\n    # Create confusion matrix heatmap\n    disp = sns.heatmap(\n        cm, annot=True, cmap='Greens',\n        annot_kws={\"size\": text_size}, fmt='g',\n        linewidths=1, linecolor='black', clip_on=False,\n        xticklabels=classes, yticklabels=classes)\n    \n    # Set title and axis labels\n    disp.set_title('Confusion Matrix', fontsize=24)\n    disp.set_xlabel('Predicted Label', fontsize=20) \n    disp.set_ylabel('True Label', fontsize=20)\n    plt.yticks(rotation=0) \n\n    # Plot confusion matrix\n    plt.show()\n    \n    return","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:10:03.534685Z","iopub.execute_input":"2025-03-24T03:10:03.535103Z","iopub.status.idle":"2025-03-24T03:10:03.543703Z","shell.execute_reply.started":"2025-03-24T03:10:03.535069Z","shell.execute_reply":"2025-03-24T03:10:03.542418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_confusion_matrix(\n    val_df.Rating - 1, \n    val_predictions, \n    figsize=(10, 10))","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:10:05.603787Z","iopub.execute_input":"2025-03-24T03:10:05.604157Z","iopub.status.idle":"2025-03-24T03:10:05.935957Z","shell.execute_reply.started":"2025-03-24T03:10:05.604128Z","shell.execute_reply":"2025-03-24T03:10:05.934677Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<br>\nThe model is able to classify the majority classes. However, the characteristics of a severly imbalanced dataset is present as the model struggles with predicting the minority classes.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id='6.3'></a>\n### Generate Classification Report","metadata":{}},{"cell_type":"code","source":"print(classification_report(val_df.Rating - 1, val_predictions))","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:10:12.544960Z","iopub.execute_input":"2025-03-24T03:10:12.545377Z","iopub.status.idle":"2025-03-24T03:10:12.576747Z","shell.execute_reply.started":"2025-03-24T03:10:12.545343Z","shell.execute_reply":"2025-03-24T03:10:12.575283Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='6.4'></a>\n### Record Classification Metrics","metadata":{}},{"cell_type":"code","source":"def generate_preformance_scores(y_true, y_pred, y_probabilities):\n    \n    model_accuracy = accuracy_score(y_true, y_pred)\n    top_2_accuracy = top_k_accuracy_score(y_true, y_probabilities, k=2)\n    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, \n                                                                                 y_pred, \n                                                                                 average=\"weighted\")\n    model_matthews_corrcoef = matthews_corrcoef(y_true, y_pred)\n    \n    print('=============================================')\n    print(f'\\nPerformance Metrics:\\n')\n    print('=============================================')\n    print(f'accuracy_score:\\t\\t{model_accuracy:.5f}\\n')\n    print('_____________________________________________')\n    print(f'top_2_accuracy_score:\\t{top_2_accuracy:.5f}\\n')\n    print('_____________________________________________')\n    print(f'precision_score:\\t{model_precision:.5f}\\n')\n    print('_____________________________________________')\n    print(f'recall_score:\\t\\t{model_recall:.5f}\\n')\n    print('_____________________________________________')\n    print(f'f1_score:\\t\\t{model_f1:.5f}\\n')\n    print('_____________________________________________')\n    print(f'matthews_corrcoef:\\t{model_matthews_corrcoef:.5f}\\n')\n    print('=============================================')\n    \n    preformance_scores = {\n        'accuracy_score': model_accuracy,\n        'top_2_accuracy_score': top_2_accuracy,\n        'precision_score': model_precision,\n        'recall_score': model_recall,\n        'f1_score': model_f1,\n        'matthews_corrcoef': model_matthews_corrcoef\n    }\n    \n    return preformance_scores","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:10:15.844195Z","iopub.execute_input":"2025-03-24T03:10:15.844651Z","iopub.status.idle":"2025-03-24T03:10:15.854736Z","shell.execute_reply.started":"2025-03-24T03:10:15.844605Z","shell.execute_reply":"2025-03-24T03:10:15.853389Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_performance = generate_preformance_scores(val_df.Rating-1, val_predictions, val_probabilities)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:10:18.760432Z","iopub.execute_input":"2025-03-24T03:10:18.760834Z","iopub.status.idle":"2025-03-24T03:10:18.785134Z","shell.execute_reply.started":"2025-03-24T03:10:18.760801Z","shell.execute_reply":"2025-03-24T03:10:18.783947Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect Competition Metric: Mean-Absolute-Error\nprint('Competition Metric Score')\nprint('=========================')\nprint(f'Train MAE:\\t{mean_absolute_error(train_new_df.rating_encoded, train_predictions):.5f}')\nprint(f'Validation MAE:\\t{mean_absolute_error(val_df.rating_encoded, val_predictions):.5f}')\nprint('=========================')","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:10:21.261445Z","iopub.execute_input":"2025-03-24T03:10:21.261843Z","iopub.status.idle":"2025-03-24T03:10:21.271972Z","shell.execute_reply.started":"2025-03-24T03:10:21.261805Z","shell.execute_reply":"2025-03-24T03:10:21.270757Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<br>\nThe model was able to achive a Matthews Correlation Coefficient of ~0.76 which is decent. A high MCC implies that the model's predictions are statistically of high quality and that the model may generalise to unseen samples. Looking at the Compition Metric, the model seems to score a low validation MAE and we can observe a significant difference between the training and validation MAE scores.\n   <br><br>\n   <b>Since the model was able to score a high MCC, we should expect a similar LB MAE score as the validation MAE.</b>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#03e3fc;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n\n<a id='7'></a>\n# 7 | Generate Submission\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>\n<br>\n\nWith the model performance evaluation complete, we need to generate the submission file.\n\n<a id='7.1'></a>\n### Preprocess Test Reviews","metadata":{}},{"cell_type":"code","source":"def predict(model, test_reviews):\n    probabilities = model.predict(test_reviews, verbose=1)\n    predictions = tf.argmax(probabilities, axis=1)\n    return probabilities, predictions","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:10:26.484259Z","iopub.execute_input":"2025-03-24T03:10:26.484739Z","iopub.status.idle":"2025-03-24T03:10:26.490450Z","shell.execute_reply.started":"2025-03-24T03:10:26.484706Z","shell.execute_reply":"2025-03-24T03:10:26.489054Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess Test Reviews\ntest_reviews = test_df['Review'].apply(text_preprocessor)\ntest_reviews.shape","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:10:27.062728Z","iopub.execute_input":"2025-03-24T03:10:27.064283Z","iopub.status.idle":"2025-03-24T03:10:29.551320Z","shell.execute_reply.started":"2025-03-24T03:10:27.064233Z","shell.execute_reply":"2025-03-24T03:10:29.550016Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='7.2'></a>\n### Generate Test Predictions","metadata":{}},{"cell_type":"code","source":"# Generate Test Predictions\ntest_probabilities, test_predictions = predict(model, test_reviews)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:10:32.933955Z","iopub.execute_input":"2025-03-24T03:10:32.934382Z","iopub.status.idle":"2025-03-24T03:11:03.783901Z","shell.execute_reply.started":"2025-03-24T03:10:32.934344Z","shell.execute_reply":"2025-03-24T03:11:03.782723Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='7.3'></a>","metadata":{}},{"cell_type":"code","source":"# Use the sample_subission dataframe to create the\n# submission csv for the test set predictions\nsubmission_df['Rating'] = test_predictions + 1 # Decode labels \n\n# View first 5 submission samples \nsubmission_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:13:38.013557Z","iopub.execute_input":"2025-03-24T03:13:38.014005Z","iopub.status.idle":"2025-03-24T03:13:38.028629Z","shell.execute_reply.started":"2025-03-24T03:13:38.013970Z","shell.execute_reply":"2025-03-24T03:13:38.027000Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View Test Predictions Ratings Distribution\nplt.figure(figsize=(14, 8))\nplt.title('Test Prediction Ratings Distribution', fontsize=20)\ntest_predictions_distribution = submission_df['Rating'].value_counts().sort_values()\n\nsns.barplot(x=test_predictions_distribution.values,\n            y=list(test_predictions_distribution.keys()),\n            orient=\"h\");\nsns.despine();","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:13:40.501931Z","iopub.execute_input":"2025-03-24T03:13:40.502385Z","iopub.status.idle":"2025-03-24T03:13:40.714601Z","shell.execute_reply.started":"2025-03-24T03:13:40.502343Z","shell.execute_reply":"2025-03-24T03:13:40.713434Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<br>\nThe test predictions resemble the train ratings distribution. However, we observe that the minority classes are overshadowed by the majority classes. This behavior should be addressed in future experiments.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id='7.4'></a>\n### Generate Submission.csv","metadata":{}},{"cell_type":"code","source":"# Create submission csv\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2025-03-24T03:13:47.044500Z","iopub.execute_input":"2025-03-24T03:13:47.044952Z","iopub.status.idle":"2025-03-24T03:13:47.100021Z","shell.execute_reply.started":"2025-03-24T03:13:47.044919Z","shell.execute_reply":"2025-03-24T03:13:47.098906Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Get Session Info (Package Requirements)","metadata":{}},{"cell_type":"code","source":"! pip -q install session_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T03:38:38.426234Z","iopub.execute_input":"2025-03-24T03:38:38.426774Z","iopub.status.idle":"2025-03-24T03:38:53.028359Z","shell.execute_reply.started":"2025-03-24T03:38:38.426726Z","shell.execute_reply":"2025-03-24T03:38:53.026951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import session_info\nsession_info.show(html=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T03:38:56.894533Z","iopub.execute_input":"2025-03-24T03:38:56.894964Z","iopub.status.idle":"2025-03-24T03:38:57.319403Z","shell.execute_reply.started":"2025-03-24T03:38:56.894926Z","shell.execute_reply":"2025-03-24T03:38:57.318128Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<center><div style='color:#ffffff;\n           display:inline-block;\n           padding: 5px 5px 5px 5px;\n           border-radius:5px;\n           background-color:#03e3fc;\n           font-size:100%;'><a href=#toc style='text-decoration: none; color:#03001C;'>‚¨ÜÔ∏è Back To Top</a></div></center>\n\n<a id='conclusion'></a>\n# <center>Conclusion</center>\n<div style=\"padding: 4px;color:white;margin:10;font-size:200%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;\"></div>\n<br>\n\nIn this notebook we built a baseline model to predict the sentiment of company reviews left by customers. We achieved this by using the Universal Sentence Encoder to generate text embeddings which where representative of the review texts, and fed these embeddings to a classification head. We achieved a Matthews Correlation Coefficient of ~0.76, which implies that the model's predictions are statistically of high quality and that the model will generalise to unseen samples.\n\nHowever, when looking at the model's confusion matrix and classification report we observe the characteristics of a severly imbalanced dataset. The model is unable to correctly predict review ratings which are between 1 and 5. Also, the classification approach might be adding to this bad model behavior as the loss function does not aim to minimize the competition metric (Mean Absolute Error). Therefore, it is recommended that an **ordinal text regression approach** should be followed as this approach will be focuse on minimizing the competition's metric and may achieve better results.\n\n<br>\n\n<div style=\"color:white;padding: 15px;color:white;margin:10;font-size:120%;text-align:center;display:fill;border-radius:10px;overflow:hidden;background-image: url(https://i.postimg.cc/fTDmwnkQ/Miaka.png); background-size: 100% auto;background-position: 0px 0px;\">\n    <center>\n    <h3><span style='color:white'>\n        I hope this notebook serves the community well!<br><br>\n        Thank you for visiting!</span>üôè</h3>\n    </center>\n</div>","metadata":{}}]}